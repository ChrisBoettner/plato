{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.table import Table\n",
    "from astropy.units import UnitsWarning\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from plato.stars import classify_stars, quality_cuts\n",
    "from plato.utils import accumulate_from_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# process targets\n",
    "\n",
    "# load targets\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UnitsWarning)\n",
    "    targets = Table.read(\"../data/raw/plato_targets.fits\").to_pandas()\n",
    "\n",
    "# make quality cuts\n",
    "targets = quality_cuts(targets, max_error=0.2)\n",
    "\n",
    "# add classification\n",
    "targets = classify_stars(targets, include_galactic_quantities=True)\n",
    "\n",
    "# rename columns\n",
    "targets.rename(\n",
    "    columns={\n",
    "        \"SOURCE_ID\": \"gaiaID_DR3\",\n",
    "        \"dr2_source_id\": \"gaiaID_DR2\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "for col in targets.columns:\n",
    "    if col.endswith(\"_error\"):\n",
    "        targets.rename(columns={col: f\"e_{col[:-6]}\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add xgboost metallicities from Andrae2023\n",
    "total_rows = 174922161\n",
    "chunksize = int(1e7)\n",
    "xgboost_data = []\n",
    "\n",
    "# load in chuncks\n",
    "for chunk in tqdm(\n",
    "    pd.read_csv(\n",
    "        \"../data/external/xgboost.csv\",\n",
    "        chunksize=chunksize,\n",
    "        compression=\"gzip\",\n",
    "        usecols=[\"source_id\", \"mh_xgboost\", \"logg_xgboost\"],\n",
    "    ),\n",
    "    total=total_rows // chunksize + 1,\n",
    "    desc=\"Processing chunks: \",\n",
    "):\n",
    "    # Select only the targets\n",
    "    filtered_chunk = chunk[chunk[\"source_id\"].isin(targets[\"gaiaID_DR3\"])]\n",
    "    xgboost_data.append(filtered_chunk)\n",
    "\n",
    "xgboost_data = pd.concat(xgboost_data).rename(columns={\"source_id\": \"gaiaID_DR3\"})\n",
    "xgboost_data = xgboost_data.assign(\n",
    "    mh_xgboost_lower=np.nan,\n",
    "    mh_xgboost_upper=np.nan,\n",
    "    logg_xgboost_lower=np.nan,\n",
    "    logg_xgboost_upper=np.nan,\n",
    ")\n",
    "targets = targets.merge(xgboost_data, on=\"gaiaID_DR3\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add [Fe/H] metallcity column, filled in following priority: gspspec,\n",
    "# gspphot, xgboost (use next possible source if previous is NaN)\n",
    "targets = accumulate_from_sources(\n",
    "    targets,\n",
    "    \"[Fe/H]\",\n",
    "    source_columns=[\"mh_gspspec\", \"mh_gspphot\", \"mh_xgboost\"],\n",
    "    drop_sources=True,\n",
    ")\n",
    "\n",
    "# add logg column, filled in following priority: gspspec, gspphot, xgboost\n",
    "targets = accumulate_from_sources(\n",
    "    targets,\n",
    "    \"logg\",\n",
    "    source_columns=[\"logg_gspspec\", \"logg_gspphot\", \"logg_xgboost\"],\n",
    "    drop_sources=True,\n",
    ")\n",
    "\n",
    "\n",
    "# rename alphafe_gspspec column (and errors) to [alpha/Fe]\n",
    "targets.rename(\n",
    "    columns={\n",
    "        \"alphafe_gspspec\": \"[alpha/Fe]\",\n",
    "        \"alphafe_gspspec_lower\": \"e_[alpha/Fe]_lower\",\n",
    "        \"alphafe_gspspec_upper\": \"e_[alpha/Fe]_upper\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add metallicities, alpha and log g from high-res spectroscopic surveys\n",
    "\n",
    "## Apogee\n",
    "apogee = Table.read(\"../data/external/apogee.fits\", hdu=1)\n",
    "# get relevant columns\n",
    "apogee = apogee[\n",
    "    [\n",
    "        \"GAIAEDR3_SOURCE_ID\",\n",
    "        \"FE_H\",\n",
    "        \"FE_H_ERR\",\n",
    "        \"ALPHA_M\",\n",
    "        \"ALPHA_M_ERR\",\n",
    "        \"LOGG\",\n",
    "        \"LOGG_ERR\",\n",
    "        \"ASPCAPFLAGS\",\n",
    "    ]\n",
    "]\n",
    "apogee_df = apogee.to_pandas()\n",
    "# qulaity cuts (drop entries where ALPHA_M or M_H is flagged)\n",
    "apogee_df = apogee_df[\n",
    "    ~apogee_df[\"ASPCAPFLAGS\"].astype(str).str.contains(\"ALPHA_M|M_H|STAR_WARN\")\n",
    "].drop(columns=\"ASPCAPFLAGS\")\n",
    "\n",
    "apogee_df = apogee_df.rename(\n",
    "    columns={\n",
    "        \"GAIAEDR3_SOURCE_ID\": \"gaiaID_DR3\",\n",
    "        \"FE_H\": \"[Fe/H]_apogee\",\n",
    "        \"FE_H_ERR\": \"e_[Fe/H]_apogee\",\n",
    "        \"ALPHA_M\": \"[alpha/M]_apogee\",\n",
    "        \"ALPHA_M_ERR\": \"e_[alpha/M]_apogee\",\n",
    "        \"LOGG\": \"logg_apogee\",\n",
    "        \"LOGG_ERR\": \"e_logg_apogee\",\n",
    "    }\n",
    ")\n",
    "# merge into targets\n",
    "targets = pd.merge(\n",
    "    targets,\n",
    "    apogee_df.drop_duplicates(subset=\"gaiaID_DR3\", keep=\"first\"),\n",
    "    on=\"gaiaID_DR3\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "## GALAH\n",
    "with warnings.catch_warnings():\n",
    "    # silence warning for log(cm.s**-2) units\n",
    "    warnings.simplefilter(\"ignore\", UnitsWarning)\n",
    "    galah = Table.read(\"../data/external/galah.fits\")\n",
    "# quality cuts\n",
    "galah = galah[galah[\"flag_sp\"] == 0]\n",
    "galah = galah[galah[\"flag_fe_h\"] == 0]\n",
    "galah = galah[galah[\"flag_alpha_fe\"] == 0]\n",
    "# get relevant columns\n",
    "galah = galah[\n",
    "    [\n",
    "        \"dr3_source_id\",\n",
    "        \"fe_h\",\n",
    "        \"e_fe_h\",\n",
    "        \"alpha_fe\",\n",
    "        \"e_alpha_fe\",\n",
    "        \"logg\",\n",
    "        \"e_logg\",\n",
    "    ]\n",
    "]\n",
    "galah_df = galah.to_pandas()\n",
    "galah_df = galah_df.rename(\n",
    "    columns={\n",
    "        \"dr3_source_id\": \"gaiaID_DR3\",\n",
    "        \"fe_h\": \"[Fe/H]_galah\",\n",
    "        \"e_fe_h\": \"e_[Fe/H]_galah\",\n",
    "        \"alpha_fe\": \"[alpha/Fe]_galah\",\n",
    "        \"e_alpha_fe\": \"e_[alpha/Fe]_galah\",\n",
    "        \"logg\": \"logg_galah\",\n",
    "        \"e_logg\": \"e_logg_galah\",\n",
    "    }\n",
    ")\n",
    "# merge into targets\n",
    "targets = pd.merge(\n",
    "    targets,\n",
    "    galah_df.drop_duplicates(subset=\"gaiaID_DR3\", keep=\"first\"),\n",
    "    on=\"gaiaID_DR3\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# process asPIC\n",
    "asPIC = Table.read(f\"../data/external/asPIC_1.1.fits\")\n",
    "asPIC = asPIC[\n",
    "    [\n",
    "        \"sourceId\",\n",
    "        \"GLON\",\n",
    "        \"GLAT\",\n",
    "        \"gaiaV\",\n",
    "        \"egaiaV\",\n",
    "        \"Gmag\",\n",
    "        \"eGmag\",\n",
    "        \"Radius\",\n",
    "        \"eRadius\",\n",
    "        \"Mass\",\n",
    "        \"eMass\",\n",
    "        \"Teff\",\n",
    "        \"eTeff\",\n",
    "        \"sourceFlag\",\n",
    "    ]\n",
    "]\n",
    "for col in asPIC.colnames:\n",
    "    asPIC[col] = asPIC[col][:, 0]\n",
    "asPIC = asPIC.to_pandas()\n",
    "\n",
    "# rename source flag\n",
    "asPIC.rename(\n",
    "    columns={\n",
    "        \"sourceId\": \"gaiaID_DR2\",\n",
    "        \"sourceFlag\": \"Stellar Type\",\n",
    "        \"egaiaV\": \"e_gaiaV\",\n",
    "        \"eGmag\": \"e_Gmag\",\n",
    "        \"eRadius\": \"e_Radius\",\n",
    "        \"eMass\": \"e_Mass\",\n",
    "        \"eTeff\": \"e_Teff\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "asPIC[\"Stellar Type\"] = asPIC[\"Stellar Type\"].map(\n",
    "    {\n",
    "        1: \"FGK\",  # FGK\n",
    "        5: \"FGK\",  # FGK and known planet host\n",
    "        2: \"M\",  # M\n",
    "        6: \"M\",  # M and known planet host\n",
    "    }\n",
    ")\n",
    "\n",
    "# match asPIC and targets on sourceId\n",
    "data = pd.merge(targets, asPIC, on=\"gaiaID_DR2\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add quadratic limb darkening coefficients for targets\n",
    "ldcs = pd.read_csv(\n",
    "    \"../data/external/PLATO_NCAM_ldcs.csv\",\n",
    "    comment=\"#\",\n",
    "    usecols=[\"Teff\", \"logg\", \"u1\", \"u2\"],\n",
    ")\n",
    "\n",
    "# use KNN to easily identify the nearest neighbour\n",
    "knn = KNeighborsRegressor(n_neighbors=1)\n",
    "knn.fit(ldcs[[\"Teff\", \"logg\"]], ldcs[[\"u1\", \"u2\"]])\n",
    "\n",
    "data = data.assign(u1=np.nan, u2=np.nan)\n",
    "data.loc[data[[\"Teff\", \"logg\"]].dropna().index, [\"u1\", \"u2\"]] = knn.predict(\n",
    "    data[[\"Teff\", \"logg\"]].dropna()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "data.to_csv(\"../data/processed/all_sky_targets.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plato",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
